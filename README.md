# MEBench
Data and code for paper "MEBench: Benchmarking Large Language Models for Cross-Document Multi-Entity Question Answering" (EMNLP 2025 Main)

MEBench is a scalable  multi-document, multi-entity benchmark  designed to systematically evaluate LLMsâ€™ capacity  to retrieve, consolidate, and reason over  scattered and dense information. 

**MEBench construction pipeline**
<div align="center">
  <img src="assets/mebench.png" width="65%" height="65%"/>
</div>

**Data Statistics**
<div align="center">
  <img src="assets/statistics.jpg" width="50%" height="50%"/>
</div>

**Leaderboard**
<div align="center">
  <img src="assets/leaderboard.jpg" width="50%" height="50%"/>
</div>

**Citation**
```
@misc{lin2025mebench,
      title={MEBench: Benchmarking Large Language Models for Cross-Document Multi-Entity Question Answering}, 
      author={Teng Lin and Yuyu Luo and Honglin Zhang and Jicheng Zhang and Chunlin Liu and Kaishun Wu and Nan Tang},
      year={2025},
      eprint={2502.18993},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2502.18993}, 
}
```
